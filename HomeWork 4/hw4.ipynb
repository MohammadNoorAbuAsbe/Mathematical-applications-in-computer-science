{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "nbgrader",
      "language": "python",
      "name": "nbgrader"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYXrQEuHXyTW"
      },
      "source": [
        "Instructions:\n",
        "1. Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"\n",
        "2. Do not add or remove cells to the document, inside the current cells you can add helper functions if you wish.\n",
        "3. Download the file and submit only one file named hw4.ipynb (Do not change file name!)\n",
        "4. Work alone, you can use any resource that was presented in class!\n",
        "5. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Runtime$\\rightarrow$Run All).\n",
        "6. Clear all outputs - Edit$\\rightarrow$Clear all outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMpDZmNRXyTb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-Ddvr3pHG4yc",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7e28609ee8536d4770a5acd79467683e",
          "grade": false,
          "grade_id": "cell-d826f2a5211905e3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Assignment 4: Differentiation and optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_pVdjMLHG4yd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "68ac4d1b8278a50275435a7cdcb117d3",
          "grade": false,
          "grade_id": "cell-2c47810b587eba40",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import sympy as sym # symbolic differentiation\n",
        "import jax          # algorithmic differentiation\n",
        "import jax.numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5acfe76de27c1d2787061d5393c66a69",
          "grade": false,
          "grade_id": "cell-8e70e66cf8309900",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xzGYUlGYXyTc"
      },
      "source": [
        "## Question 1: Differentiation\n",
        "\n",
        "Function \n",
        "\n",
        "$$f(a, b) = \\frac 2 b cos(a) \\exp \\left( - \\frac {a^2} {b^2}\\right)$$\n",
        "\n",
        "is given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7c66850d2769109ae45e3fe88d5c41cd",
          "grade": false,
          "grade_id": "cell-8f63e6aeedb5c09b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aXmB2P2vXyTd"
      },
      "source": [
        "1. Derive the partial derivatives of $f(a, b)$ by $a$ and $b$.\n",
        "\n",
        "    $$\\frac {\\partial f} {\\partial a} = $$\n",
        "    $$\\frac {\\partial f} {\\partial b} = $$\n",
        "\n",
        "Implement the function f_partial_derviatives:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "a - first symbol\n",
        "b - second symbol\n",
        "return value - a tuple (dfa, dfb) such that dfa is the partial derivatives of f by a,\n",
        "               and dfb is the partial derivative of f by b\n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "80600e68fbd9a8fa065f02b6bd9b0baf",
          "grade": false,
          "grade_id": "cell-fcc26ea16c7fcc6e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Bt4hqb2GXyTd"
      },
      "source": [
        "def f_partial_derviatives_symbol(a, b):\n",
        "    # YOUR CODE HERE\n",
        "    f = (2/b)*sym.cos(a)*sym.exp(-((a**2)/(b**2)))\n",
        "    return (sym.diff(f, a), sym.diff(f, b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "786889cd0849242f4f30bd9a443f5dfd",
          "grade": true,
          "grade_id": "cell-910c1caba85f9efe",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "70X0oYqnXyTe"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q1.1 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'f_partial_derviatives_symbol' function..\\n \")\n",
        "a=sym.Symbol('a')\n",
        "b=sym.Symbol('b')\n",
        "f_deriv_a, f_deriv_b = f_partial_derviatives_symbol(a, b)\n",
        "assert isinstance(f_deriv_a, sym.Basic)\n",
        "assert isinstance(f_deriv_b, sym.Basic)\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "19358409e08ee5150391dee7d8699370",
          "grade": false,
          "grade_id": "cell-5cbb42199eaf95ef",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "MN3Qkp1IXyTe"
      },
      "source": [
        "2. Derive the same derivatives by a and b, but using using algorithmic differentiation this time (with `jax`).\n",
        "\n",
        "Implement the function f_partial_derviatives_algo:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "return value - a tuple (dfa, dfb) such that dfa is the partial derivatives of f by a,\n",
        "               and dfb is the partial derivative of f by b\n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e4ce43ce84665c666cc4367cd4d75f16",
          "grade": false,
          "grade_id": "cell-68ca280308184043",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3mdpRFjUXyTf"
      },
      "source": [
        "def f_partial_derviatives_algo():\n",
        "    # YOUR CODE HERE\n",
        "    dfunction1a = jax.grad(function1,0)\n",
        "    dfunction1b = jax.grad(function1,1)\n",
        "    return (dfunction1a,dfunction1b)\n",
        "\n",
        "def function1(a,b):\n",
        "    return (2/b)*np.cos(a)*np.exp(-((a**2)/(b**2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed15d3176f8ccbaf8deebdb1f1b6e175",
          "grade": true,
          "grade_id": "cell-bf714cb7381ed64d",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "91XTOjo-XyTf"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q1.2 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'f_partial_derviatives_algo' function..\\n \")\n",
        "a=1.\n",
        "b=1.\n",
        "f_deriv_a, f_deriv_b = f_partial_derviatives_algo()\n",
        "assert callable(f_deriv_a)\n",
        "assert callable(f_deriv_b)\n",
        "assert f_deriv_a(a, b) < 0\n",
        "assert f_deriv_b(a, b) > 0\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e234237cf36aabcca272f450f45a879f",
          "grade": false,
          "grade_id": "cell-2f70f1b68a8a3397",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3xzJRRJ9XyTf"
      },
      "source": [
        "3. Plot the following:\n",
        "\n",
        "a. One subplot for both:\n",
        "\n",
        "  * $f(a, 10)$, $\\frac {\\partial f(a, 10)} {\\partial a}$ for range $a \\in [-20, 20]$, \n",
        "  \n",
        "b. A seperate subplot below the first subplot for:\n",
        "  \n",
        "  * $f(10, b)$, $\\frac {\\partial f(10, b)} {\\partial b}$ for range $b \\in [1, 100]$.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ee87320d9fe95a363022b938145f6f5d",
          "grade": true,
          "grade_id": "cell-7c4c505ac0fa2e26",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LtfniyKQXyTg"
      },
      "source": [
        "# show you plots here:\n",
        "# YOUR CODE HERE\n",
        "a = np.linspace(-20, 20, 1000)\n",
        "f_deriv_a, f_deriv_b = f_partial_derviatives_algo()\n",
        "plt.plot(a, [function1(a,10) for a in a], label=\"f(a,10)\")\n",
        "plt.plot(a, [f_deriv_a(a,10) for a in a], label=\"d f/d a\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\"                                 \")\n",
        "b = np.linspace(1, 100, 1000)\n",
        "plt.plot(b, [function1(10,b) for b in b], label=\"f(10,b)\")\n",
        "plt.plot(b, [f_deriv_b(10,b) for b in b], label=\"d f/d b\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f8033254cb6c569ca12a1c2b7d1e6a6b",
          "grade": false,
          "grade_id": "cell-d41582685181dca1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FhKmgAcPXyTg"
      },
      "source": [
        "4. Implement a function for approximate numerical differentiation, given the difference size $h$.\n",
        "\n",
        "Implement the function f_partial_derviatives_numeric_at_a_b:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "a - first parameter of f\n",
        "b - second parameter of f\n",
        "h - the change of the variable to approximate the derivative by\n",
        "return value - a tuple that contains partial derivatives of f by a and b approximated at (a,b)\n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a05425f84b97d3f347c1f3c82b4a2214",
          "grade": false,
          "grade_id": "cell-1dc99d4eaa406e89",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "hYx3JMg6XyTh"
      },
      "source": [
        "def f_partial_derviatives_numeric_at_a_b(a, b, h):\n",
        "    # YOUR CODE HERE\n",
        "    fa = (function1(a + h, b) - function1(a, b)) / h\n",
        "    fb = (function1(a, b + h) - function1(a, b)) / h\n",
        "    return (fa,fb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4eb71eab53355a28e5d61b8d34358c6b",
          "grade": true,
          "grade_id": "cell-f9a81b3184e827ab",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "z56XLhUkXyTh"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q1.4 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'f_partial_derviatives_numeric' function..\\n \")\n",
        "a=1.\n",
        "b=1.\n",
        "f_deriv_a_at_a_b, f_deriv_b_at_a_b = f_partial_derviatives_numeric_at_a_b(a, b, 1e-12)\n",
        "assert f_deriv_a_at_a_b < 0\n",
        "assert f_deriv_b_at_a_b > 0\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b269e94a9851cc15881552aec439ff36",
          "grade": false,
          "grade_id": "cell-6f154f57c695e08e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S0gex10bXyTh"
      },
      "source": [
        "5. Find the best difference size $h$ for differentiating\n",
        "   * $f(a, b)$ by $a$.\n",
        "   * $f(a, b)$ by $b$.\n",
        "   \n",
        "The best difference size minimizes the absolute error of numerical differentiation relative to the exact differentiation.\n",
        "   \n",
        "Implement the function f_deriv_by_a_optimize_h:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "a - first parameter of f\n",
        "b - second parameter of f\n",
        "return value - the best difference h to do numeric differentiation of f(a,b) by a\n",
        "</pre> \n",
        "\n",
        "Implement the function f_deriv_by_b_optimize_h:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "a - first parameter of f\n",
        "b - second parameter of f\n",
        "return value - the best difference h to do numeric differentiation of f(a,b) by b\n",
        "</pre> \n",
        "\n",
        "Non-Mandatory: draw plots of the abolute error by h \n",
        "(it's a good practice for you to validate that the results make sense)\n",
        "\n",
        "* Allowed relative tolerance is up to 100 (so if best h=1e-04, any value between 1e-02 and 1e-06 is allowed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "920a2cce72f823d4b3e31c29c48ac272",
          "grade": false,
          "grade_id": "cell-1bd9e03b2999d29f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "y43U-pVkXyTi"
      },
      "source": [
        "def f_deriv_by_a_optimize_h(a, b):\n",
        "    # YOUR CODE HERE\n",
        "    h = 1\n",
        "    optimized = 0.1\n",
        "    numeric = f_partial_derviatives_numeric_at_a_b(a,b,h)\n",
        "    algo = f_partial_derviatives_algo()[0](a, b)\n",
        "    opt = abs(algo-numeric[0])\n",
        "    while numeric[0]!= 0:\n",
        "      h = h * 0.1\n",
        "      numeric = f_partial_derviatives_numeric_at_a_b(a,b,h)\n",
        "      dif = abs(algo-numeric[0])\n",
        "      if dif < opt:\n",
        "        optimized = h\n",
        "        opt = dif\n",
        "    return optimized "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d4df9aee6d7ee97ade14d13e6743552f",
          "grade": false,
          "grade_id": "cell-aafc10f1c62a469a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "UAN7y5HJXyTj"
      },
      "source": [
        "def f_deriv_by_b_optimize_h(a, b):\n",
        "    # YOUR CODE HERE\n",
        "    h = 1\n",
        "    optimized = 0.1\n",
        "    numeric = f_partial_derviatives_numeric_at_a_b(a,b,h)\n",
        "    algo = f_partial_derviatives_algo()[1](a, b)\n",
        "    opt = abs(algo-numeric[1])\n",
        "    while numeric[1]!= 0:\n",
        "      h = h * 0.1\n",
        "      numeric = f_partial_derviatives_numeric_at_a_b(a,b,h)\n",
        "      dif = abs(algo-numeric[1])\n",
        "      if dif < opt:\n",
        "        optimized = h\n",
        "        opt = dif\n",
        "    return optimized "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8ef50f28ac8b9242fc89ed47f93fbfec",
          "grade": true,
          "grade_id": "cell-65a8e984bababd74",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "un6ixXjfXyTj"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q1.5 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'f_partial_derviatives_numeric' function..\\n \")\n",
        "assert f_deriv_by_a_optimize_h(4.,1.) < 1e-05\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b289fe25b06385665a5b837b797c3a07",
          "grade": false,
          "grade_id": "cell-89f9f7e53914ff2a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "o5l1m0bfXyTj"
      },
      "source": [
        "## Question 2: Optimization\n",
        "\n",
        "### Logistic regression\n",
        "\n",
        "For a trial group of care bares (https://en.wikipedia.org/wiki/Care_Bears), amount of sugar in blood and the event of sleepiness are given as a list of pairs (sugar, sleep) (1 corresponds to sleepiness):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2761d5c514a701224fc4f10bcdb23660",
          "grade": false,
          "grade_id": "cell-5a43cf6adc2e93f6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2-VOlcvgXyTj"
      },
      "source": [
        "care_bears = \\\n",
        "[(15.5, 1), (6.5, 0), (49.5, 1), (17.5, 1), (43.0, 1), (42.5, 1), (11.5, 0), \n",
        " (23.5, 0), (17.5, 1), (11.0, 0), (13.5, 0), (9.0, 0), (10.5, 1), (31.0, 0),\n",
        " (30.0, 1), (30.0, 1), (47.0, 1), (44.5, 1), (15.5, 0), (46.0, 0), (3.0, 1),\n",
        " (42.0, 0), (67.0, 0), (2.5, 1)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a2fa05f56266446f3bd0b564aa1ba866",
          "grade": false,
          "grade_id": "cell-c03650829efcf55d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Kf1WKF4TXyTk"
      },
      "source": [
        "We want to predict care bear sleepiness based on the amount of the sugar. The prediction function is\n",
        "\n",
        "$$sleep = \\frac{sugar}{100} \\ge threshold.$$\n",
        "\n",
        "The loss for this _classification_ problem is called log_loss:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "& L = -\\sum_{i=1}^N (sleep_i \\log p_i + (1 - sleep_i) \\log (1 - p_i))\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "where the predicted probability is:\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "& p_i = \\frac 1 {1 + \\exp(threshold - \\frac{sugar_i}{100})}\n",
        "\\end{aligned}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0216222f9a19f37697b90d9833949521",
          "grade": false,
          "grade_id": "cell-07381a88c978e1fe",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UY9ZFwY1XyTk"
      },
      "source": [
        "1. Implement the loss as a function of the threshold.\n",
        "\n",
        "Implement the function care_bare_classification_loss:\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "threashold - the treshold of the classification\n",
        "data - the data in the format of list of tuples\n",
        "return value - the loss as defined above\n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "69bd9fadddfc9fc5f58027576dc2b87f",
          "grade": false,
          "grade_id": "cell-da55752637b08a28",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LF3x0HydXyTl"
      },
      "source": [
        "def care_bare_classification_loss(threshold, data=care_bears):\n",
        "    # YOUR CODE HERE\n",
        "    sum = 0\n",
        "    for i in range(len(data)):\n",
        "      pi = 1/(1+np.exp(threshold-(data[i][0]/100)))\n",
        "      value = data[i][1]*np.log(pi) + (1 - data[i][1])*np.log(1-pi)\n",
        "      sum = sum + value\n",
        "    return - sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "da185926db5a90fbc8331d6360160a4b",
          "grade": true,
          "grade_id": "cell-1e51ea7117c6868f",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xY1Se9TAXyTl"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q2.1 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'loss' function..\\n \")\n",
        "assert care_bare_classification_loss(0.5) > 0\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d5a3342bab2991778a107e2b972e1a14",
          "grade": false,
          "grade_id": "cell-895b78311fff1ecb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "btfDCVo2XyTl"
      },
      "source": [
        "2. Plot the loss and the derivative of the loss\n",
        "\n",
        "Implement a function care_bare_classification_loss_deriv\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "threashold - the treshold of the classification\n",
        "return value - the derviative of the loss\n",
        "</pre> \n",
        "\n",
        "Now plot the loss and the derivative of the loss by the threshold in the range $threshold \\in (0.01, 0.99)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "321af0fa872fe6d6d960c10b34b0776c",
          "grade": false,
          "grade_id": "cell-9e65d966d3fe2745",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Imp8BOT8XyTm"
      },
      "source": [
        "def care_bare_classification_loss_deriv(threshold):\n",
        "    # YOUR CODE HERE\n",
        "    dloss = jax.grad(care_bare_classification_loss)\n",
        "    return dloss(threshold, data=care_bears)\n",
        "\n",
        "threshold = np.linspace(0.01, 0.99, 100)\n",
        "plt.plot(threshold, [care_bare_classification_loss(threshold, care_bears) for threshold in threshold], label=\"loss\")\n",
        "plt.plot(threshold, [care_bare_classification_loss_deriv(threshold) for threshold in threshold], label=\"dloss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "72cc6e1e04635a89ce9dcb48a820ee57",
          "grade": true,
          "grade_id": "cell-779160dbb3fa5fbf",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "iZetuHD1XyTm"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q2.2 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'loss' function..\\n \")\n",
        "assert care_bare_classification_loss_deriv(0.5) > 0\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c163230004110b517584ac8434ab4f2a",
          "grade": false,
          "grade_id": "cell-e344ecace17a1cef",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PTws6oiBXyTm"
      },
      "source": [
        "3. Find the best threshold using gradient descent - use jax for the dervative!\n",
        "\n",
        "Implement a function loss_best_threshold_gd\n",
        "\n",
        "<pre>\n",
        "return value - the best threshold according to the loss function \n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73887ee8fb7806c138e378d461a64629",
          "grade": false,
          "grade_id": "cell-252b27d3b325b9e9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cdO0IHP3XyTm"
      },
      "source": [
        "def loss_best_threshold_gd():\n",
        "    # YOUR CODE HERE\n",
        "    return gd(care_bare_classification_loss, 0.1,0.1,0.995,100)\n",
        "\n",
        "def gd(f, x0, step=0.1, decay=0.995, niter=100):\n",
        "  \"\"\"approximates minimum of f starting from x0\n",
        "  \"\"\"\n",
        "  df = jax.grad(f)\n",
        "  x = x0\n",
        "  for i in range(niter):\n",
        "    x -= df(x)*step\n",
        "    step *= decay\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c597730834adc49a5d793a86872c7122",
          "grade": true,
          "grade_id": "cell-7e98f7a26296840a",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "L0QdyYvkXyTn"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q2.3 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'loss' function..\\n \")\n",
        "best_threshold = loss_best_threshold_gd()\n",
        "assert best_threshold > 0\n",
        "assert care_bare_classification_loss(best_threshold) < care_bare_classification_loss(best_threshold+0.5)\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bd00dbd95d003dfbe2f038641e3ce6df",
          "grade": false,
          "grade_id": "cell-79ab1e6f1ab4b201",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4S4jFsxOXyTn"
      },
      "source": [
        "4. Find the best threshold using Newton's method.\n",
        "\n",
        "Implement a function loss_best_threshold_newton\n",
        "\n",
        "<pre>\n",
        "return value - the best threshold according to the loss function \n",
        "</pre> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b9f23f0547c078b78e338ee9c1e5a18a",
          "grade": false,
          "grade_id": "cell-94bc44c21f3a1b2f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ZHmsUL9vXyTo"
      },
      "source": [
        "def loss_best_threshold_newton():\n",
        "    # YOUR CODE HERE\n",
        "    return newton(care_bare_classification_loss, 0.1,10)\n",
        "\n",
        "def newton(f, x0, niter=10):\n",
        "  df = jax.grad(f)\n",
        "  ddf = jax.grad(df)\n",
        "  x = x0\n",
        "  for i in range(niter):\n",
        "    x = x - df(x)/ddf(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73d2453233567d76e959b54952ada334",
          "grade": true,
          "grade_id": "cell-e03d19aad1688287",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "TsMe8sFmXyTo"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q2.4 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'loss' function..\\n \")\n",
        "best_threshold = loss_best_threshold_newton()\n",
        "assert best_threshold > 0\n",
        "assert care_bare_classification_loss(best_threshold) < care_bare_classification_loss(best_threshold+0.5)\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a8a558fbb7427640df46a87857950f77",
          "grade": false,
          "grade_id": "cell-d908eaa6fd1bc3f7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QFlFAHa1XyTo"
      },
      "source": [
        "5. Using the best threshold found, how many sleep=1 cases were misclassified?\n",
        "\n",
        "Implement a function misclassified_sleeps\n",
        "\n",
        "<pre>\n",
        "Input parameters:\n",
        "data - the data in the format of list of tuples\n",
        "use_newton - True if using Newton, False if using GD\n",
        "return value - how many sleep=1 cases were missclassified\n",
        "</pre> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fa9602ebce442326ee3c07e3346eb339",
          "grade": false,
          "grade_id": "cell-cfdeadebe7272bf1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dBV41VBGXyTo"
      },
      "source": [
        "def misclassified_sleeps(use_newton=False, data=care_bears):\n",
        "    # YOUR CODE HERE\n",
        "    threshold = 0\n",
        "    missclassified = 0\n",
        "    if use_newton == True:\n",
        "      threshold = loss_best_threshold_newton()\n",
        "    else:\n",
        "      threshold = loss_best_threshold_gd()\n",
        "    for i in range(len(data)):\n",
        "      if data[i][1] == 1:\n",
        "        if data[i][0]/100 < threshold:\n",
        "          missclassified = missclassified +1\n",
        "    return missclassified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f56662b27ccd2a2a332e790e876136ef",
          "grade": true,
          "grade_id": "cell-a6bf63dd69b400cc",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Qb2nJJdbXyTp"
      },
      "source": [
        "# --------------------------- RUN THIS TEST CODE CELL -------------------------------------\n",
        "# Q2.5 --- Test your implementation:\n",
        "# ---------------------------\n",
        "print(\"Testing the implementation of the 'loss' function..\\n \")\n",
        "missclassified_sleep_count_gd = misclassified_sleeps(False)\n",
        "missclassified_sleep_count_newton = misclassified_sleeps(True)\n",
        "assert missclassified_sleep_count_gd >= 0\n",
        "assert missclassified_sleep_count_newton >= 0\n",
        "print (\"good job!\\nSanity tests passed. There are additional hidden tests...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}